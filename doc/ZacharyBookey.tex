\documentclass{ccr15}
% PACKAGES ---------------------------------------------------------------
\usepackage{amsfonts,amsmath,graphicx,subfigure}
\usepackage{listings}
% ADD YOUR OWN PACKAGES HERE ---------------------------------------------
%\usepackage{someotherpackage}
% DEFINITIONS ------------------------------------------------------------
% ADD YOUR OWN DEFINITIONS HERE ------------------------------------------
% BE SURE TO PREFACE LABEL WITH YOUR OWN INITIALS (ZAB in this example) --
% This controls the table-of-contents entry in the proceedings. Edit it
% to include your article title followed by the authors' names, as shown.
\addcontentsline{toc}{chapter}{Performance Portable HPCG\\
{\em Z.B.\ Student and I.D.\ Mentor and S.R \ Mentor}}
\pagestyle{myheadings}
\thispagestyle{plain}
% This gives the running head. Usually you list a shortened version of
% your article title (unless it's already very short) along with
% the author's names, as shown.
\markboth{Performance Portable HPCG}{Z.B.\ Student and I.D.\ Mentor and S.R \ Mentor}
% Put your article title in here
\title{Performance Portable HPCG}
% List each author, their affiliation, and their e-mail address, as shown.
\author{Zachary A.\ Bookey\thanks{Saint John's University, zabookey@csbsju.edu} \and Irina P.\ Demeshko\thanks{Sandia National Laboratories,
ipdemes@sandia.gov} \and Sivasankaran Rajamanickam\thanks{Sandia National Laboratories, srajama@sandia.gov}}
\begin{document}
\maketitle
% Include your abstract here.
\begin{abstract}
%Irina comment: we can't use acronyms in abstarct
The High Performance Conjugate Gradients Benchmark is an international project to create a
more appropriate benchmark test for the world's largest computers. The current LINPACK benchmark,
which is the standard for measuring the performance of the top 500 fastest computers in the
world, is moving computers in a direction that is no longer beneficial to many important
parallel applications. In this project we are developing a version of HPCG, using the Kokkos
package found in Trilinos, that can be optimally executed across several distinct high
performance computing architectures. This new code demonstrates an efficient programming
approach that can be adopted by other programmers to write portable high performance software.
\end{abstract}
\section{Introduction}

After generations of using the High Performance Linpack (HPL) benchmark to measure the
performance of large computers it became necessary to use another benchmark to help better the
direction that super computers were headed to more accurately reflect the types of applications
that these machines were running.

HPL is a simple program that factors and solves a large dense system of linear equations using
Gaussian Elimination with partial pivoting.
While dense matrix - matrix multiplication and related kernels are commonly used in scientific applications,
they are not representative of all the operations usually performed in scientific computing:
computations with lower computation-to-data-access ratios(computational intensity) and with
irregular memory access are also very common.

The High Performance Conjugate Gradient (HPCG) was created to fill the gap that HPL had created.
HPCG uses a preconditioned conjugate gradient to solve a system of equations, that executes both
dense computations with high computational intensity and computations with low computational intensity such as sparse matrix-matrix multiplications.

Original, MPI-only,  HPCG benchmark doesn't exploit full parallelism available on existing Supercomputers which makes it unfair to use it for performance measurement.
And the goal of our project was to create a performance portable version of HPCG that gives reasonable performance on all existing supercomputers, by using Kokkos library from Trilinos.

% Irina comment: I think we can add related work here


\section{HPCG}
HPCG is a new and
upcoming benchmark test to rank the worlds largest computers. On top
of solving a large system of equations, HPCG also features a more irregular data access pattern
so that data access affects results as well as matrix computations.

HPCG begins by creating a symmetric positive definite matrix and it's corresponding multi grid
to be used in the preconditioning phase. For the preconditioner it uses a Symmetric Gauss-Seidel
forward sweep and back sweep to solve the lower and upper triangular matrices. For the actual
solve of $A x = b$, HPCG uses the conjugate gradient method after the preconditioning phase.
HPCG runs in seven major phases.
\begin{enumerate}
\item \textbf{Problem Setup:} This is the beginning of HPCG and is where we construct the
geometry that is used to generate the problem. HPCG generates a symmetric, positive definite,
sparse matrix with up to 27 nonzero entries per row depending on the local location of the row.
\item \textbf{Validation Testing:} This portion of the program is to make sure any changes
made produce valid results. Specifically it checks to make sure that both the unpreconditioned
and preconditioned conjugate gradient converge in around 12 and 2 iterations respectively. It
also makes sure that after performing both a sparse matrix vector multiplication and a symmetric
Gauss-Seidel sweep that we preserve symmetry by using two pseudorandomly filled vectors and
performing simple operations that should be zero due to the nature of our symmetric matrix A.
\item \textbf{Reference Sparse Matrix Vector Multiplication and Multigrid Timing:} This
portion of the code times how long it takes to perform the reference versionts of SPMV and
Symmetric Gauss-Seidel.
\item \textbf{Reference Conjugate Gradient Timing:} Here we run 50 iterations of the reference
version of the conjugate gradient method and record the resulting residual. This residual must be
attained by the optimized version of conjugate gradient no matter how many iterations are
required.
\item \textbf{Optimized Conjugate Gradient Setup:} Runs one set of the optimized conjugate
gradient and determines the number of iterations required to reach the residual found before.
Then figures out how mamy times to reach the desired residual to fill in the requested benchmark
time.
\item \textbf{Optimized Conjugate Gradient Timing:} Runs the optimized conjugate gradient the
required amount of times. Records time for each timed section to report out later.
\item \textbf{Report Results:} Writes out log files for debugging and creates the .yaml file
to display the results which can then be submitted if all the requirements are met.
\end{enumerate}
HPCG gives you the option to run with MPI, OpenMP, both, or in serial. Running with MPI adds an
extra dimension to the problem and requires processes to exchange values on their borders to
perform. This results in a tradeoff between more overhead and more parallelism.
\section{Kokkos}
As different computer architectures are better with certain applications than others it has
become increasingly difficult to write code that will perform well across many different types of
architectures. One solution to this problem is the C++ package, Kokkos. Kokkos acts as a wrapper
around your code to allow you to specify at compile time where and how you want to run your
application. Currently Kokkos supports the following execution spaces:
\begin{itemize}
\item Serial
\item PThreads
\item OpenMP
\item Cuda
\end{itemize}
Kokkos has two main features, views and parallel kernels. A view is essentially a wrapper around
an array of data that gives you the option to specify which execution space you want to store the
data on and allows you to choose what sort of memory access traits you wish this data to have.
Views also handle their own memory management via reference counting so that the view
automatically deallocates itself when all of the variables that reference it go out of scope,
thus making memory management much simpler across multiple devices.

There are three main parallel kernels: parallel\_for, parallel\_reduce, and parallel\_scan. All 
of these serve their own purpose and act as wrappers over how you would execute a section of 
code in parallel over the respective execution space. For all of the parallel kernels you 
initiate the kernel by passing in a functor that performs the desired parallel operation, as of 
host to device.
Parallel\_for is simply a generic for loop that will run all of the context of the loop in
parallel. This works well for parallel kernels like vector addition. Parallel\_reduce is for
simultaneously updating a single value, this function guarantees that you avoid race conditions
with the updated values. Parallel\_reduce works well for parallel kernels like finding the dot
product of two vectors. Parallel\_scan is for taking a view and creating a running sum of values
to replace the values of the view. Although parallel\_scan is useful it was only really needed
for setup phases in our HPCG.
Kokkos allows for nested parallelism that involves creating a league of teams of threads.
With this tool, a developer could launch a parallel\_reduce kernel that uses a parallel\_for to
update some value that later gets added to the value that was initially included to be reduced.
Although this has not been implemented in our project yet, there are places in our code that
could and should benefit from nested parallelism and thus we intend to include it at a later
time.
\section{HPCG + Kokkos}
The goal for our project was to create a version of HPCG that uses Kokkos features to be able to
produce valid results across many architectures without sacrificing performance. First we needed
to rewrite the parallel kernels to use Kokkos parallel kernels, then we needed to restructure 
all of the code to work with Kokkos views, and finally we needed to make a few optimizations to
avoid an unecessary amount of memory copying from the host to the device and vice versa.
\subsection{Kokkos re-factoring}
Rewriting the parallel kernels involved replacing the parallel loops with the correct type of
Kokkos parallel kernel. This part of re-factoring was heavily focused on converting the
computation algorithms into functors and lambdas. Completing this task didn't affect portability
at all and due to some Kokkos restrictions actually caused a slight reduction of performance in
the function ComputeResidual. Other kernels would later have to be changed to accomodate the fact
that data was stored on a device but was being run on the host.

Restructuring the code involved a whole rewrite of HPCG to change how all of the structs stored
their values. We replaced every array that would be used in a parallel kernel with an appropriate
view. Once this was functional we had to go back to some of the compute algorithms and change
how the data was accessed as to not try to access device data from the host or the
other way around.

While restructuring we decided to change how our SparseMatrix stored the data and implemented it
as a sort of overlying structure on top of a Kokkos CSRMatrix. This change required us to again
go back and change how most of our computational kernels worked and created a noticeable increase
in performance. At this point the code was functional across all of Kokkos execution spaces but
took a severe performance hit while trying to run on Cuda.

Computing the preconditioner using a symmetric Gauss-Seidel was initially done in serial and thus
moving to Kokkos required us to copy memory from the device to the host every time we ran it,
which was the reason performance was lost. We tried implementing many different ways to perform
a sweep of symmetric Gauss-Seidel in parallel to eliminate the need of copying data. We
implemented an inexact solve, a level solve, and a colouring algorithm.

The inexact solve works by performing a few triangular matrix solves. We split our matrix $A$ into
parts $L$, $U$, and $D$ where $L$ is the lower triangular matrix including the diagonal, $U$ is the upper
triangular matrix including the diagonal and $D$ is the diagonal. The inexact solve is done by solving three
different equations. Given our problem $A x = b$, to simulate a symmetric Gauss-Seidel sweep we solve
$L z = b$ then $D w = z$ and finally $U x = w$. All of these solves are done using a parallel Jacobi solve.
Although this version of symmetric Gauss-Seidel is run in parallel and avoid all device to host memory
copies it still performs slowly since each Jacobi solve needs to be run a certain amount of time depending
on problem density and results in more parallel kernels being launched than is ideal.

Our level solve algorithm splits our matrix $A$ into parts $L$, $U$, and $D$ where $L$ is the
lower triangular part of $A$ that includes the diagonal, $U$ is similar to $L$ since it is just
the upper triangular part of $A$ that includes the diagonal. $D$ is just the diagonal of A. For
this algorithm we introduce another data structure, levels, to our SparseMatrix that stores all 
of the data needed for sorting the matrix. When we optimize the problem we find dependencies in
solving for $L$ and sort based on those dependencies in a way that a row will only be solved if
all of it's dependencies have been solved. We repeat this process for solving for $A$ and store
all of this data into levels. Now when we compute our Symmetric Gauss-Seidel we solve just like
we do for the inexact solve except we start by solving for the rows in level 1 in parallel and
then the rows in level 2 in parallel and repeat until we solve all of our levels. In the end we
have a Symmetric Gauss-Seidel that has introduced a deal of parallelism and performs well
compared to the original implementation.

At the time of writing our coloring algorithm does not work. We are in the process of debugging and
determining what a good fix will be. The algorithm works by coloring our matrix $A$ so that all rows
with a certain color have no dependencies on one another. This way we can run a sweep of symmetric
Gauss-Seidel in parallel over each color. While similar to the level solve, this method requires more iterations
to converge due to the fact that although no two rows in the same color depend on each other it is likely
that they depend on a row in a color that will be solved in a later iteration.

\subsection{Performance evaluation result}
Performance results coming soon!
%Irina comment: I would remove "Future Work" as a section and just add the contect into conclusion
%\section{Future Work}
\section{Conclusion}
In the end we have worked towards creating a version of HPCG that works alongside the Kokkos
package found in Trilinos. This version of HPCG will be a useful for being able to run the reference
version of HPCG out of the box and not need to configure the code to be compatible with the
specific machine being benchmarked.

Our work here is not completed but we have made great headway on this project and currently have
code that produces similar results across all of the Kokkos execution spaces. In the future we want to
fix a few performance bottlenecks and utilize hierarchical parallelism to fully take advantage of Kokkos
kernels.We also want to fix our coloring algorithm for the symmetric Gauss-Seidel so we can choose at
compile time which algorithm to use for preconditioning. It will be interesting to compare performance
between all of our preconditioning algorithms.
\section{Acknowledgements}
%Irina comment: I'll fill this section later 

\bibliographystyle{siam}
% Edit the line below to be your first and last names.
\nocite{ZAB:Mentor05}
\nocite{ZAB:TechHPCG}
\bibliography{ZacharyBookey}
% Edit FirstnameLastname below to be your first and last names, but leave the line commented out.
% This line will help me merge bibliographies for the proceedings.
%\input{ZacharyBookey/ZacharyBookey.bbl}
\end{document}
